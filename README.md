# Earnings Event Trading Agent (EETA) v2.0

## Multi-Agent Reinforcement Learning for Intelligent Earnings-Based Trading

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ¯ Overview

EETA is a sophisticated multi-agent reinforcement learning system that learns optimal trading strategies around corporate earnings announcements. The system combines:

- **Deep Q-Networks (DQN)** for position selection
- **Thompson Sampling** for position sizing under uncertainty
- **Cost-Aware Orchestration** of specialized analysis agents
- **Walk-Forward Validation** to prevent look-ahead bias

### Key Features

- âœ… 5 trading actions (equity + simulated volatility plays)
- âœ… 36 carefully engineered features
- âœ… Walk-forward validation methodology
- âœ… SPY benchmark comparison
- âœ… Ablation studies proving component value
- âœ… Built-in risk controls with kill switch

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COST-AWARE ORCHESTRATOR                          â”‚
â”‚  Conditionally runs agents based on confidence levels               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HISTORICAL   â”‚   â”‚   SENTIMENT   â”‚   â”‚    MARKET     â”‚
â”‚    AGENT      â”‚   â”‚    AGENT      â”‚   â”‚    AGENT      â”‚
â”‚               â”‚   â”‚               â”‚   â”‚               â”‚
â”‚ â€¢ Beat rate   â”‚   â”‚ â€¢ News tone   â”‚   â”‚ â€¢ VIX level   â”‚
â”‚ â€¢ Avg move    â”‚   â”‚ â€¢ Attention   â”‚   â”‚ â€¢ SPY trend   â”‚
â”‚ â€¢ Consistency â”‚   â”‚ â€¢ Revisions   â”‚   â”‚ â€¢ Regime      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    36-DIM STATE       â”‚
                â”‚    VECTOR             â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  POSITION SELECTORâ”‚               â”‚   SIZE OPTIMIZER  â”‚
â”‚      (DQN)        â”‚               â”‚ (Thompson Sampling)â”‚
â”‚                   â”‚               â”‚                   â”‚
â”‚  36 â†’ 128 â†’ 64 â†’ 5â”‚               â”‚  Beta distributionsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   RISK CONTROLLER     â”‚
                â”‚   (Hard Limits)       â”‚
                â”‚                       â”‚
                â”‚  â€¢ Max 5% per trade   â”‚
                â”‚  â€¢ Daily loss 3%      â”‚
                â”‚  â€¢ Drawdown 10%       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/earnings-trading-agent.git
cd earnings-trading-agent

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Build Dataset

```python
from src.data import build_dataset

# Build earnings dataset (uses free yfinance data)
dataset = build_dataset(
    start_date="2019-01-01",
    end_date="2024-12-01",
    cache_path="data/processed/earnings_dataset.parquet"
)

print(f"Dataset size: {len(dataset)} earnings events")
```

### Train the Agent

```python
from src.training import EETATrainer
from src.utils import get_config

# Load configuration
config = get_config()

# Initialize trainer
trainer = EETATrainer(
    data=dataset,
    config=config._raw,
    experiment_name="experiment_001"
)

# Train
results = trainer.train(n_episodes=100)

print(f"Best Sharpe: {results['best_sharpe']:.2f}")
```

### Run Walk-Forward Validation

```python
from src.training import WalkForwardValidator, create_agent_factory, train_fold, test_fold

# Create validator
validator = WalkForwardValidator(
    data=dataset,
    min_train_years=3,
    test_years=1
)

# Run validation
results = validator.run_validation(
    agent_factory=create_agent_factory(config._raw),
    train_fn=train_fold,
    test_fn=test_fold,
    config=config._raw
)

print(f"Mean Test Sharpe: {results['aggregate']['mean_sharpe']:.2f}")
```

---

## ğŸ“Š Action Space

| ID | Action | Description | When Optimal |
|----|--------|-------------|--------------|
| 0 | NO_TRADE | Skip this earnings | Low confidence |
| 1 | LONG_STOCK | Buy shares | High confidence bullish |
| 2 | SHORT_STOCK | Short shares | High confidence bearish |
| 3 | LONG_VOL | Long volatility (straddle-like) | Big move expected |
| 4 | SHORT_VOL | Short volatility (condor-like) | Small move expected |

---

## ğŸ“ˆ State Space (36 Features)

### Historical Features (0-11)
- Beat rate, average moves, consistency, guidance impact, trends

### Sentiment Features (12-19)  
- News sentiment, volume, analyst revisions, attention

### Market Features (20-27)
- VIX level/percentile, SPY momentum, market regime

### Technical Features (28-33)
- RSI, trend strength, volume ratio, momentum

### Meta Features (34-35)
- Signal agreement, overall confidence

---

## ğŸ“ Project Structure

```
earnings-trading-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/           # Analysis agents
â”‚   â”‚   â”œâ”€â”€ historical_agent.py
â”‚   â”‚   â”œâ”€â”€ sentiment_agent.py
â”‚   â”‚   â”œâ”€â”€ market_agent.py
â”‚   â”‚   â””â”€â”€ orchestrator.py
â”‚   â”œâ”€â”€ rl/               # Reinforcement learning
â”‚   â”‚   â”œâ”€â”€ dqn.py
â”‚   â”‚   â”œâ”€â”€ thompson.py
â”‚   â”‚   â””â”€â”€ replay_buffer.py
â”‚   â”œâ”€â”€ environment/      # Trading environment
â”‚   â”‚   â”œâ”€â”€ trading_env.py
â”‚   â”‚   â””â”€â”€ action_simulator.py
â”‚   â”œâ”€â”€ data/             # Data pipeline
â”‚   â”‚   â”œâ”€â”€ sources.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â””â”€â”€ dataset_builder.py
â”‚   â”œâ”€â”€ training/         # Training infrastructure
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ curriculum.py
â”‚   â”‚   â””â”€â”€ walk_forward.py
â”‚   â”œâ”€â”€ evaluation/       # Evaluation
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ benchmarks.py
â”‚   â”‚   â””â”€â”€ ablation.py
â”‚   â”œâ”€â”€ risk/             # Risk management
â”‚   â”‚   â””â”€â”€ controller.py
â”‚   â””â”€â”€ utils/            # Utilities
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ logging.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml
â”œâ”€â”€ data/
â”œâ”€â”€ experiments/
â”œâ”€â”€ demo/
â””â”€â”€ notebooks/
```

---

## ğŸ”¬ Evaluation

### Benchmarks

The system is compared against:
- Buy & Hold SPY
- Random Agent
- Always Long
- Momentum Strategy
- Beat Rate Strategy

### Ablation Studies

Proves each component's contribution:
- Full System (baseline)
- No Historical Agent
- No Sentiment Agent
- No Thompson Sampling
- Random Actions

---

## âš ï¸ Risk Controls

Hard-coded safety limits that **cannot** be overridden:

| Control | Limit | Purpose |
|---------|-------|---------|
| Max Position | 5% | Prevent concentration |
| Daily Loss | 3% | Daily circuit breaker |
| Max Drawdown | 10% | Capital preservation |
| Consecutive Losses | 5 | Cool-down trigger |

---

## ğŸ“š References

- [DQN Paper](https://arxiv.org/abs/1312.5602) - Mnih et al. 2013
- [Thompson Sampling](https://arxiv.org/abs/1707.02038) - Tutorial
- [Walk-Forward Validation](https://quantstart.com/articles/Walk-Forward-Optimisation/) - QuantStart

---

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Please read our contributing guidelines first.

---

## ğŸ“§ Contact

For questions and feedback, please open an issue on GitHub.
